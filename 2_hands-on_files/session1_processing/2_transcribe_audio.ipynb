{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51caaaa7-ac22-423d-a89f-efe13e7f79ab",
   "metadata": {},
   "source": [
    "# Transcribe Audio\n",
    "\n",
    "In this script, we load the audio file and transcribe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c5cc1-6148-4de9-a198-c222ce682dce",
   "metadata": {},
   "source": [
    "### Option 1: Process a single file\n",
    "\n",
    "Select a single audio file as input and specify the output (JSON and text file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f815478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9ef72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def read_huggingface_token(token_file: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Reads the Hugging Face token from a specified file path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Path object from the given file path\n",
    "        path = Path(os.path.expanduser(token_file))\n",
    "        \n",
    "        # Check if the file exists before trying to read it\n",
    "        if path.exists():\n",
    "            return path.read_text().strip()\n",
    "        else:\n",
    "            print(f\"Error: The token file does not exist at '{path}'.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the token file: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae49dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# Your script now uses this function\n",
    "token_path = \"~/.cache/huggingface/token\"\n",
    "hf_token = read_huggingface_token(token_path)\n",
    "\n",
    "if hf_token:\n",
    "    print(\"Hugging Face token successfully loaded.\")\n",
    "    # Now you can use hf_token in your function calls\n",
    "else:\n",
    "    print(\"Failed to load Hugging Face token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f931b2-7dea-4752-906c-cee5b39afa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transcribe audio.wav\n",
    "\"\"\"\n",
    "\n",
    "import openwillis.transcribe as owt\n",
    "import json\n",
    "\n",
    "def transcribe_audio(audio_file, output_json, output_text):\n",
    "    \"\"\"Transcribe audio file using OpenWillis\"\"\"\n",
    "    try:\n",
    "        # Use OpenWillis to transcribe the audio\n",
    "        # Returns: (transcript_json, transcript_text)\n",
    "        result = owt.speech_transcription_whisper(\n",
    "            filepath=audio_file,\n",
    "            model=\"large-v2\",      # Best model, takes longer\n",
    "            compute_type=\"int16\",  # Default for CPU\n",
    "            device_type=\"cpu\",     # Use CPU \n",
    "            batch_size=16,         # Default batch size\n",
    "            hf_token= hf_token,  # Replace with your token\n",
    "            language=\"\",           # Auto-detection if not specified\n",
    "            min_speakers = 1,      # Speaker diarization possible\n",
    "            max_speakers = 1\n",
    "        )\n",
    "        \n",
    "        # Unpack the result\n",
    "        transcript_json, transcript_text = result\n",
    "        \n",
    "        # Save the JSON transcript (detailed word-by-word)\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcript_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save the text transcript (simple string)\n",
    "        with open(output_text, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "        \n",
    "        print(f\" Transcription saved:\")\n",
    "        print(f\"  JSON: {output_json}\")\n",
    "        print(f\"  Text: {output_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_file}: {e}\")\n",
    "\n",
    "# Define input and output files\n",
    "audio_file = \"output/RAVDESS/audio_only/01-01-01-01-01-01-01.wav\"     # Input audio file from previous step\n",
    "json_output = \"output/RAVDESS/transcripts/01-01-01-01-01-01-01.json\"  # Detailed transcript\n",
    "text_output = \"output/RAVDESS/transcripts/01-01-01-01-01-01-01.txt\"   # Simple text transcript\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio\n",
    "transcribe_audio(audio_file, json_output, text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3ca99-3b2f-4553-b00d-ab76439d3da3",
   "metadata": {},
   "source": [
    "### Option 2: Process all files in a folder\n",
    "\n",
    "Select a folder with audio files as input and specify your output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043888e4-cbe2-4821-ae48-51753df5d386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Example_Video_audio.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../opt/anaconda3/envs/openwillis_3.1/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      " Transcription saved:\n",
      "  JSON: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_JSON_Folder/Example_Video_audio_transcript.json\n",
      "  Text: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_Text_Folder/Example_Video_audio_transcript.txt\n",
      "Processing Example_Audio.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../opt/anaconda3/envs/openwillis_3.1/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      " Transcription saved:\n",
      "  JSON: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_JSON_Folder/Example_Audio_transcript.json\n",
      "  Text: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_Text_Folder/Example_Audio_transcript.txt\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transcribe all audio files in a directory\n",
    "\"\"\"\n",
    "\n",
    "import openwillis.transcribe as owt\n",
    "import json\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file, output_json, output_text):\n",
    "    \"\"\"Transcribe single audio file using OpenWillis\"\"\"\n",
    "    try:\n",
    "        # Use OpenWillis to transcribe the audio\n",
    "        # Returns: (transcript_json, transcript_text)\n",
    "        result = owt.speech_transcription_whisper(\n",
    "            filepath=audio_file,\n",
    "            model=\"large-v2\",      # Best model, takes longer\n",
    "            compute_type=\"int16\",  # Default for CPU\n",
    "            device_type=\"cpu\",     # Use CPU \n",
    "            batch_size=16,         # Default batch size\n",
    "            hf_token=hf_token,  # Replace with your token\n",
    "            language=\"en\",         # Specifying recommended, else Auto-detection\n",
    "            min_speakers=1,        # Speaker diarization possible\n",
    "            max_speakers=1\n",
    "        )\n",
    "        \n",
    "        # Unpack the result\n",
    "        transcript_json, transcript_text = result\n",
    "        \n",
    "        # Save the JSON transcript \n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcript_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save the text transcript (simple string)\n",
    "        with open(output_text, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "        \n",
    "        print(f\" Transcription saved:\")\n",
    "        print(f\"  JSON: {output_json}\")\n",
    "        print(f\"  Text: {output_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_file}: {e}\")\n",
    "\n",
    "def process_directory(input_dir, json_output_dir, text_output_dir):\n",
    "    \"\"\"Transcribe all audio files in directory\"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    os.makedirs(text_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all files in the input directory\n",
    "    for file in os.listdir(input_dir):\n",
    "        # Only process WAV files\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "            \n",
    "        # Create full paths for input and output files\n",
    "        audio_file = os.path.join(input_dir, file)\n",
    "        base_name = os.path.splitext(file)[0]  # Remove .wav extension\n",
    "        json_output = os.path.join(json_output_dir, f\"{base_name}_transcript.json\")\n",
    "        text_output = os.path.join(text_output_dir, f\"{base_name}_transcript.txt\")\n",
    "        \n",
    "        # Skip if transcription files already exist\n",
    "        if os.path.exists(json_output) and os.path.exists(text_output):\n",
    "            print(f\"Skipping {file}: transcription already exists\")\n",
    "            continue\n",
    "            \n",
    "        # Process the audio file\n",
    "        print(f\"Processing {file}...\")\n",
    "        transcribe_audio(audio_file, json_output, text_output)\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = \"output/RAVDESS/audio_only\" # Replace with your folder\n",
    "json_output_folder = \"output/RAVDESS/transcripts/JSON_files\"\n",
    "text_output_folder = \"output/RAVDESS/transcripts/txt_files\"\n",
    "\n",
    "# Process all audio files in the directory\n",
    "process_directory(input_folder, json_output_folder, text_output_folder)\n",
    "print(\"All files processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openwillis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
