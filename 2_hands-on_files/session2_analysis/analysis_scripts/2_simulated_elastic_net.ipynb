{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ff197a",
   "metadata": {},
   "source": [
    "# 1. Elastic Net for feature selection\n",
    "\n",
    "In this script, we will:\n",
    "1. Load and preprocess the dataset.\n",
    "2. Fit Elastic Net model.\n",
    "3. Visualize top features per emotion class.\n",
    "4. Identify features with zero coefficient value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf92a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393d701",
   "metadata": {},
   "source": [
    "### Task 1: Load the CSV file\n",
    "Same as in script of correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1790734a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>GHQ_somatic_symptoms</th>\n",
       "      <th>GHQ_anxiety_insomnia</th>\n",
       "      <th>GHQ_social_dysfunction</th>\n",
       "      <th>GHQ_severe_depression</th>\n",
       "      <th>GHQ_total</th>\n",
       "      <th>STADI_Trait_agitation</th>\n",
       "      <th>STADI_Trait_worry</th>\n",
       "      <th>STADI_Trait_euthymia</th>\n",
       "      <th>STADI_Trait_dysthymia</th>\n",
       "      <th>STADI_Trait_total</th>\n",
       "      <th>PSS_total</th>\n",
       "      <th>LEC_exposure</th>\n",
       "      <th>MIMIS_burden</th>\n",
       "      <th>MIMIS_exposure</th>\n",
       "      <th>CTQ_total</th>\n",
       "      <th>condition</th>\n",
       "      <th>f0_mean</th>\n",
       "      <th>f0_stddev</th>\n",
       "      <th>f0_range</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_stddev</th>\n",
       "      <th>f1_range</th>\n",
       "      <th>f2_mean</th>\n",
       "      <th>f2_stddev</th>\n",
       "      <th>f2_range</th>\n",
       "      <th>f3_mean</th>\n",
       "      <th>f3_stddev</th>\n",
       "      <th>f3_range</th>\n",
       "      <th>f4_mean</th>\n",
       "      <th>f4_stddev</th>\n",
       "      <th>f4_range</th>\n",
       "      <th>loudness_mean</th>\n",
       "      <th>loudness_stddev</th>\n",
       "      <th>loudness_range</th>\n",
       "      <th>hnr_mean</th>\n",
       "      <th>hnr_stddev</th>\n",
       "      <th>hnr_range</th>\n",
       "      <th>jitter</th>\n",
       "      <th>jitter_abs</th>\n",
       "      <th>jitter_rap</th>\n",
       "      <th>jitter_ppq5</th>\n",
       "      <th>jitter_ddp</th>\n",
       "      <th>shimmer</th>\n",
       "      <th>shimmer_db</th>\n",
       "      <th>shimmer_apq3</th>\n",
       "      <th>shimmer_apq5</th>\n",
       "      <th>shimmer_apq11</th>\n",
       "      <th>shimmer_dda</th>\n",
       "      <th>gne_ratio</th>\n",
       "      <th>mfcc1_mean</th>\n",
       "      <th>mfcc2_mean</th>\n",
       "      <th>mfcc3_mean</th>\n",
       "      <th>mfcc4_mean</th>\n",
       "      <th>mfcc5_mean</th>\n",
       "      <th>mfcc6_mean</th>\n",
       "      <th>mfcc7_mean</th>\n",
       "      <th>mfcc8_mean</th>\n",
       "      <th>mfcc9_mean</th>\n",
       "      <th>mfcc10_mean</th>\n",
       "      <th>mfcc11_mean</th>\n",
       "      <th>mfcc12_mean</th>\n",
       "      <th>mfcc13_mean</th>\n",
       "      <th>mfcc14_mean</th>\n",
       "      <th>mfcc1_var</th>\n",
       "      <th>mfcc2_var</th>\n",
       "      <th>mfcc3_var</th>\n",
       "      <th>mfcc4_var</th>\n",
       "      <th>mfcc5_var</th>\n",
       "      <th>mfcc6_var</th>\n",
       "      <th>mfcc7_var</th>\n",
       "      <th>mfcc8_var</th>\n",
       "      <th>mfcc9_var</th>\n",
       "      <th>mfcc10_var</th>\n",
       "      <th>mfcc11_var</th>\n",
       "      <th>mfcc12_var</th>\n",
       "      <th>mfcc13_var</th>\n",
       "      <th>mfcc14_var</th>\n",
       "      <th>cpp_mean</th>\n",
       "      <th>cpp_var</th>\n",
       "      <th>spir</th>\n",
       "      <th>dur_med</th>\n",
       "      <th>dur_mad</th>\n",
       "      <th>silence_ratio</th>\n",
       "      <th>rel_f0_sd</th>\n",
       "      <th>rel_se0_sd</th>\n",
       "      <th>anger_mean</th>\n",
       "      <th>disgust_mean</th>\n",
       "      <th>fear_mean</th>\n",
       "      <th>happiness_mean</th>\n",
       "      <th>sadness_mean</th>\n",
       "      <th>surprise_mean</th>\n",
       "      <th>neutral_mean</th>\n",
       "      <th>AU01_mean</th>\n",
       "      <th>AU02_mean</th>\n",
       "      <th>AU04_mean</th>\n",
       "      <th>AU05_mean</th>\n",
       "      <th>AU06_mean</th>\n",
       "      <th>AU07_mean</th>\n",
       "      <th>AU09_mean</th>\n",
       "      <th>AU10_mean</th>\n",
       "      <th>AU11_mean</th>\n",
       "      <th>AU12_mean</th>\n",
       "      <th>AU14_mean</th>\n",
       "      <th>AU15_mean</th>\n",
       "      <th>AU17_mean</th>\n",
       "      <th>AU20_mean</th>\n",
       "      <th>AU23_mean</th>\n",
       "      <th>AU24_mean</th>\n",
       "      <th>AU25_mean</th>\n",
       "      <th>AU26_mean</th>\n",
       "      <th>AU28_mean</th>\n",
       "      <th>AU43_mean</th>\n",
       "      <th>mouth_openness_mean</th>\n",
       "      <th>anger_std</th>\n",
       "      <th>disgust_std</th>\n",
       "      <th>fear_std</th>\n",
       "      <th>happiness_std</th>\n",
       "      <th>sadness_std</th>\n",
       "      <th>surprise_std</th>\n",
       "      <th>neutral_std</th>\n",
       "      <th>AU01_std</th>\n",
       "      <th>AU02_std</th>\n",
       "      <th>AU04_std</th>\n",
       "      <th>AU05_std</th>\n",
       "      <th>AU06_std</th>\n",
       "      <th>AU07_std</th>\n",
       "      <th>AU09_std</th>\n",
       "      <th>AU10_std</th>\n",
       "      <th>AU11_std</th>\n",
       "      <th>AU12_std</th>\n",
       "      <th>AU14_std</th>\n",
       "      <th>AU15_std</th>\n",
       "      <th>AU17_std</th>\n",
       "      <th>AU20_std</th>\n",
       "      <th>AU23_std</th>\n",
       "      <th>AU24_std</th>\n",
       "      <th>AU25_std</th>\n",
       "      <th>AU26_std</th>\n",
       "      <th>AU28_std</th>\n",
       "      <th>AU43_std</th>\n",
       "      <th>mouth_openness_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>neutral</td>\n",
       "      <td>96.604305</td>\n",
       "      <td>106.481187</td>\n",
       "      <td>497.527041</td>\n",
       "      <td>536.424917</td>\n",
       "      <td>233.754904</td>\n",
       "      <td>2464.771816</td>\n",
       "      <td>1695.857691</td>\n",
       "      <td>296.608117</td>\n",
       "      <td>2733.114515</td>\n",
       "      <td>2791.178599</td>\n",
       "      <td>298.995644</td>\n",
       "      <td>2166.045138</td>\n",
       "      <td>3932.643447</td>\n",
       "      <td>357.076609</td>\n",
       "      <td>5397.640299</td>\n",
       "      <td>54.584391</td>\n",
       "      <td>7.335533</td>\n",
       "      <td>36.334180</td>\n",
       "      <td>7.256327</td>\n",
       "      <td>5.929404</td>\n",
       "      <td>30.862217</td>\n",
       "      <td>0.035086</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.023556</td>\n",
       "      <td>0.052829</td>\n",
       "      <td>0.169198</td>\n",
       "      <td>1.577511</td>\n",
       "      <td>0.077511</td>\n",
       "      <td>0.101986</td>\n",
       "      <td>0.152519</td>\n",
       "      <td>0.235058</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>-412.656647</td>\n",
       "      <td>125.328049</td>\n",
       "      <td>-5.162876</td>\n",
       "      <td>19.649237</td>\n",
       "      <td>2.307594</td>\n",
       "      <td>8.645128</td>\n",
       "      <td>-12.219556</td>\n",
       "      <td>-11.588177</td>\n",
       "      <td>-18.409943</td>\n",
       "      <td>-18.645103</td>\n",
       "      <td>-10.612516</td>\n",
       "      <td>-13.641768</td>\n",
       "      <td>-6.321719</td>\n",
       "      <td>-4.944730</td>\n",
       "      <td>2344.435547</td>\n",
       "      <td>790.002625</td>\n",
       "      <td>530.800659</td>\n",
       "      <td>245.813293</td>\n",
       "      <td>313.791046</td>\n",
       "      <td>165.966782</td>\n",
       "      <td>140.408020</td>\n",
       "      <td>87.357391</td>\n",
       "      <td>69.960289</td>\n",
       "      <td>93.729332</td>\n",
       "      <td>58.714436</td>\n",
       "      <td>48.906387</td>\n",
       "      <td>38.506908</td>\n",
       "      <td>35.027893</td>\n",
       "      <td>15.295692</td>\n",
       "      <td>2.597815</td>\n",
       "      <td>0.201667</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.315040</td>\n",
       "      <td>57.097033</td>\n",
       "      <td>1.095334</td>\n",
       "      <td>0.133440</td>\n",
       "      <td>2.549506</td>\n",
       "      <td>0.608424</td>\n",
       "      <td>0.078704</td>\n",
       "      <td>6.842891</td>\n",
       "      <td>0.420406</td>\n",
       "      <td>7.288031</td>\n",
       "      <td>82.211540</td>\n",
       "      <td>0.379876</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>0.193788</td>\n",
       "      <td>0.287283</td>\n",
       "      <td>0.240030</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.191009</td>\n",
       "      <td>0.078408</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.253859</td>\n",
       "      <td>0.501327</td>\n",
       "      <td>0.484803</td>\n",
       "      <td>0.527295</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.495145</td>\n",
       "      <td>0.405418</td>\n",
       "      <td>0.482940</td>\n",
       "      <td>0.382482</td>\n",
       "      <td>0.147255</td>\n",
       "      <td>0.156937</td>\n",
       "      <td>23.126518</td>\n",
       "      <td>7.225237</td>\n",
       "      <td>1.952985</td>\n",
       "      <td>0.227613</td>\n",
       "      <td>24.319393</td>\n",
       "      <td>2.266029</td>\n",
       "      <td>5.975174</td>\n",
       "      <td>35.813168</td>\n",
       "      <td>0.102583</td>\n",
       "      <td>0.050298</td>\n",
       "      <td>0.115985</td>\n",
       "      <td>0.051084</td>\n",
       "      <td>0.170119</td>\n",
       "      <td>0.274974</td>\n",
       "      <td>0.103052</td>\n",
       "      <td>0.140787</td>\n",
       "      <td>0.489667</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.210862</td>\n",
       "      <td>0.152521</td>\n",
       "      <td>0.105481</td>\n",
       "      <td>0.470924</td>\n",
       "      <td>0.135590</td>\n",
       "      <td>0.205980</td>\n",
       "      <td>0.391346</td>\n",
       "      <td>0.189271</td>\n",
       "      <td>0.104020</td>\n",
       "      <td>0.224458</td>\n",
       "      <td>4.779605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>happy</td>\n",
       "      <td>127.019532</td>\n",
       "      <td>88.005173</td>\n",
       "      <td>425.839279</td>\n",
       "      <td>535.865682</td>\n",
       "      <td>213.257234</td>\n",
       "      <td>2072.103543</td>\n",
       "      <td>1690.325368</td>\n",
       "      <td>322.428175</td>\n",
       "      <td>3054.901427</td>\n",
       "      <td>2763.748723</td>\n",
       "      <td>277.013150</td>\n",
       "      <td>3066.132075</td>\n",
       "      <td>3859.689490</td>\n",
       "      <td>373.231877</td>\n",
       "      <td>5411.600029</td>\n",
       "      <td>53.034700</td>\n",
       "      <td>7.398659</td>\n",
       "      <td>34.115776</td>\n",
       "      <td>11.240782</td>\n",
       "      <td>6.419543</td>\n",
       "      <td>34.996242</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>0.159914</td>\n",
       "      <td>1.442173</td>\n",
       "      <td>0.071833</td>\n",
       "      <td>0.101419</td>\n",
       "      <td>0.147259</td>\n",
       "      <td>0.210641</td>\n",
       "      <td>0.715506</td>\n",
       "      <td>-452.657104</td>\n",
       "      <td>135.154694</td>\n",
       "      <td>1.564904</td>\n",
       "      <td>17.994049</td>\n",
       "      <td>-0.509994</td>\n",
       "      <td>4.637099</td>\n",
       "      <td>-9.283381</td>\n",
       "      <td>-7.039497</td>\n",
       "      <td>-10.919061</td>\n",
       "      <td>-14.119902</td>\n",
       "      <td>-11.469385</td>\n",
       "      <td>-10.784728</td>\n",
       "      <td>-6.068041</td>\n",
       "      <td>-7.607950</td>\n",
       "      <td>1910.520508</td>\n",
       "      <td>700.867432</td>\n",
       "      <td>282.492523</td>\n",
       "      <td>199.471848</td>\n",
       "      <td>176.008224</td>\n",
       "      <td>113.036934</td>\n",
       "      <td>150.768936</td>\n",
       "      <td>93.646530</td>\n",
       "      <td>60.236416</td>\n",
       "      <td>49.465313</td>\n",
       "      <td>52.001652</td>\n",
       "      <td>39.587254</td>\n",
       "      <td>47.852798</td>\n",
       "      <td>38.696766</td>\n",
       "      <td>15.027884</td>\n",
       "      <td>2.370696</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.152167</td>\n",
       "      <td>44.560748</td>\n",
       "      <td>0.739442</td>\n",
       "      <td>0.138167</td>\n",
       "      <td>0.650940</td>\n",
       "      <td>8.482101</td>\n",
       "      <td>0.104265</td>\n",
       "      <td>32.010799</td>\n",
       "      <td>0.510894</td>\n",
       "      <td>0.246688</td>\n",
       "      <td>31.518047</td>\n",
       "      <td>0.291626</td>\n",
       "      <td>0.301858</td>\n",
       "      <td>0.116367</td>\n",
       "      <td>0.347408</td>\n",
       "      <td>0.618265</td>\n",
       "      <td>0.468531</td>\n",
       "      <td>0.290034</td>\n",
       "      <td>0.629238</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.794495</td>\n",
       "      <td>0.563615</td>\n",
       "      <td>0.574896</td>\n",
       "      <td>0.407324</td>\n",
       "      <td>0.473404</td>\n",
       "      <td>0.346378</td>\n",
       "      <td>0.193672</td>\n",
       "      <td>0.787273</td>\n",
       "      <td>0.429520</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>0.334740</td>\n",
       "      <td>27.791473</td>\n",
       "      <td>2.189342</td>\n",
       "      <td>35.176575</td>\n",
       "      <td>1.204603</td>\n",
       "      <td>34.631630</td>\n",
       "      <td>2.453246</td>\n",
       "      <td>5.760518</td>\n",
       "      <td>37.301800</td>\n",
       "      <td>0.079605</td>\n",
       "      <td>0.100976</td>\n",
       "      <td>0.049846</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.267469</td>\n",
       "      <td>0.470088</td>\n",
       "      <td>0.103786</td>\n",
       "      <td>0.341909</td>\n",
       "      <td>0.452735</td>\n",
       "      <td>0.268431</td>\n",
       "      <td>0.093725</td>\n",
       "      <td>0.159801</td>\n",
       "      <td>0.102256</td>\n",
       "      <td>0.500625</td>\n",
       "      <td>0.162797</td>\n",
       "      <td>0.149417</td>\n",
       "      <td>0.380655</td>\n",
       "      <td>0.188924</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.280063</td>\n",
       "      <td>5.905929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>26</td>\n",
       "      <td>stress</td>\n",
       "      <td>104.352416</td>\n",
       "      <td>80.628550</td>\n",
       "      <td>492.859823</td>\n",
       "      <td>549.499292</td>\n",
       "      <td>195.576501</td>\n",
       "      <td>1973.228026</td>\n",
       "      <td>1652.595697</td>\n",
       "      <td>300.956057</td>\n",
       "      <td>2752.069793</td>\n",
       "      <td>2756.298906</td>\n",
       "      <td>270.743336</td>\n",
       "      <td>3123.928277</td>\n",
       "      <td>3895.376444</td>\n",
       "      <td>387.713151</td>\n",
       "      <td>5294.750771</td>\n",
       "      <td>56.983224</td>\n",
       "      <td>7.407861</td>\n",
       "      <td>36.032660</td>\n",
       "      <td>9.277295</td>\n",
       "      <td>5.459930</td>\n",
       "      <td>34.988818</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>1.434752</td>\n",
       "      <td>0.065842</td>\n",
       "      <td>0.106139</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.200735</td>\n",
       "      <td>0.711139</td>\n",
       "      <td>-410.697601</td>\n",
       "      <td>137.463089</td>\n",
       "      <td>-5.219544</td>\n",
       "      <td>22.447281</td>\n",
       "      <td>-2.982574</td>\n",
       "      <td>9.057162</td>\n",
       "      <td>-4.631145</td>\n",
       "      <td>-13.301534</td>\n",
       "      <td>-14.168283</td>\n",
       "      <td>-16.331743</td>\n",
       "      <td>-12.601930</td>\n",
       "      <td>-9.062191</td>\n",
       "      <td>-3.341748</td>\n",
       "      <td>-5.156540</td>\n",
       "      <td>2468.172363</td>\n",
       "      <td>1086.665771</td>\n",
       "      <td>315.618103</td>\n",
       "      <td>245.449173</td>\n",
       "      <td>208.679886</td>\n",
       "      <td>147.195374</td>\n",
       "      <td>96.288376</td>\n",
       "      <td>66.058853</td>\n",
       "      <td>62.870209</td>\n",
       "      <td>68.456337</td>\n",
       "      <td>53.763882</td>\n",
       "      <td>53.418858</td>\n",
       "      <td>56.459240</td>\n",
       "      <td>43.430695</td>\n",
       "      <td>15.055896</td>\n",
       "      <td>2.248205</td>\n",
       "      <td>0.419921</td>\n",
       "      <td>0.6890</td>\n",
       "      <td>0.063182</td>\n",
       "      <td>54.503616</td>\n",
       "      <td>0.773881</td>\n",
       "      <td>0.130775</td>\n",
       "      <td>0.708864</td>\n",
       "      <td>7.020399</td>\n",
       "      <td>0.563129</td>\n",
       "      <td>31.234692</td>\n",
       "      <td>0.713657</td>\n",
       "      <td>1.046582</td>\n",
       "      <td>58.599697</td>\n",
       "      <td>0.381426</td>\n",
       "      <td>0.282718</td>\n",
       "      <td>0.151235</td>\n",
       "      <td>0.326344</td>\n",
       "      <td>0.667726</td>\n",
       "      <td>0.681592</td>\n",
       "      <td>0.463432</td>\n",
       "      <td>0.515825</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.727933</td>\n",
       "      <td>0.761967</td>\n",
       "      <td>0.577076</td>\n",
       "      <td>0.400158</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.366442</td>\n",
       "      <td>0.269149</td>\n",
       "      <td>0.749651</td>\n",
       "      <td>0.569153</td>\n",
       "      <td>0.042612</td>\n",
       "      <td>0.238092</td>\n",
       "      <td>23.209839</td>\n",
       "      <td>2.455595</td>\n",
       "      <td>11.329882</td>\n",
       "      <td>8.890807</td>\n",
       "      <td>40.812943</td>\n",
       "      <td>1.911544</td>\n",
       "      <td>8.728185</td>\n",
       "      <td>38.862835</td>\n",
       "      <td>0.071536</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>0.082285</td>\n",
       "      <td>0.062929</td>\n",
       "      <td>0.206310</td>\n",
       "      <td>0.495407</td>\n",
       "      <td>0.129207</td>\n",
       "      <td>0.326406</td>\n",
       "      <td>0.479352</td>\n",
       "      <td>0.299969</td>\n",
       "      <td>0.114262</td>\n",
       "      <td>0.158375</td>\n",
       "      <td>0.081024</td>\n",
       "      <td>0.471398</td>\n",
       "      <td>0.112794</td>\n",
       "      <td>0.115976</td>\n",
       "      <td>0.267835</td>\n",
       "      <td>0.137593</td>\n",
       "      <td>0.018114</td>\n",
       "      <td>0.269494</td>\n",
       "      <td>6.210818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>neutral</td>\n",
       "      <td>124.357467</td>\n",
       "      <td>101.024482</td>\n",
       "      <td>499.330235</td>\n",
       "      <td>563.682759</td>\n",
       "      <td>184.679481</td>\n",
       "      <td>2581.450413</td>\n",
       "      <td>1730.330420</td>\n",
       "      <td>301.335320</td>\n",
       "      <td>3046.016603</td>\n",
       "      <td>2809.216860</td>\n",
       "      <td>253.389106</td>\n",
       "      <td>3233.679339</td>\n",
       "      <td>3922.743203</td>\n",
       "      <td>389.741167</td>\n",
       "      <td>5333.386736</td>\n",
       "      <td>60.188131</td>\n",
       "      <td>7.761690</td>\n",
       "      <td>40.079754</td>\n",
       "      <td>11.292166</td>\n",
       "      <td>6.020655</td>\n",
       "      <td>35.320026</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.038756</td>\n",
       "      <td>0.136277</td>\n",
       "      <td>1.333635</td>\n",
       "      <td>0.056344</td>\n",
       "      <td>0.082231</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>0.702688</td>\n",
       "      <td>-363.402191</td>\n",
       "      <td>121.782951</td>\n",
       "      <td>-5.162876</td>\n",
       "      <td>13.216275</td>\n",
       "      <td>-5.863396</td>\n",
       "      <td>2.906855</td>\n",
       "      <td>-14.926833</td>\n",
       "      <td>-16.520691</td>\n",
       "      <td>-13.818157</td>\n",
       "      <td>-23.299971</td>\n",
       "      <td>-12.890169</td>\n",
       "      <td>-12.276170</td>\n",
       "      <td>-6.864491</td>\n",
       "      <td>-8.935539</td>\n",
       "      <td>2195.217529</td>\n",
       "      <td>1005.213684</td>\n",
       "      <td>431.079529</td>\n",
       "      <td>385.493713</td>\n",
       "      <td>295.572388</td>\n",
       "      <td>125.941345</td>\n",
       "      <td>242.493271</td>\n",
       "      <td>90.305038</td>\n",
       "      <td>64.390907</td>\n",
       "      <td>119.272141</td>\n",
       "      <td>69.967506</td>\n",
       "      <td>62.288139</td>\n",
       "      <td>52.287842</td>\n",
       "      <td>36.745750</td>\n",
       "      <td>15.612209</td>\n",
       "      <td>3.015392</td>\n",
       "      <td>0.381191</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>46.489476</td>\n",
       "      <td>0.767778</td>\n",
       "      <td>0.128089</td>\n",
       "      <td>0.303205</td>\n",
       "      <td>5.993415</td>\n",
       "      <td>0.274243</td>\n",
       "      <td>33.066822</td>\n",
       "      <td>3.269015</td>\n",
       "      <td>7.288031</td>\n",
       "      <td>39.839859</td>\n",
       "      <td>0.545608</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.161678</td>\n",
       "      <td>0.245133</td>\n",
       "      <td>0.610585</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.336865</td>\n",
       "      <td>0.407223</td>\n",
       "      <td>0.589520</td>\n",
       "      <td>0.483533</td>\n",
       "      <td>0.500428</td>\n",
       "      <td>0.289023</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>0.363231</td>\n",
       "      <td>0.290635</td>\n",
       "      <td>0.747445</td>\n",
       "      <td>0.352010</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.379027</td>\n",
       "      <td>23.690867</td>\n",
       "      <td>0.536156</td>\n",
       "      <td>16.174517</td>\n",
       "      <td>1.232962</td>\n",
       "      <td>33.196033</td>\n",
       "      <td>2.266029</td>\n",
       "      <td>23.911682</td>\n",
       "      <td>38.218197</td>\n",
       "      <td>0.089155</td>\n",
       "      <td>0.090863</td>\n",
       "      <td>0.080697</td>\n",
       "      <td>0.034277</td>\n",
       "      <td>0.228250</td>\n",
       "      <td>0.458867</td>\n",
       "      <td>0.090659</td>\n",
       "      <td>0.354915</td>\n",
       "      <td>0.484948</td>\n",
       "      <td>0.240953</td>\n",
       "      <td>0.130662</td>\n",
       "      <td>0.153249</td>\n",
       "      <td>0.088911</td>\n",
       "      <td>0.477791</td>\n",
       "      <td>0.109224</td>\n",
       "      <td>0.165184</td>\n",
       "      <td>0.334715</td>\n",
       "      <td>0.174493</td>\n",
       "      <td>0.036487</td>\n",
       "      <td>0.323725</td>\n",
       "      <td>5.725777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>happy</td>\n",
       "      <td>135.461952</td>\n",
       "      <td>101.699554</td>\n",
       "      <td>493.071836</td>\n",
       "      <td>551.354266</td>\n",
       "      <td>206.599329</td>\n",
       "      <td>2133.555782</td>\n",
       "      <td>1717.891940</td>\n",
       "      <td>360.038652</td>\n",
       "      <td>2821.529345</td>\n",
       "      <td>2752.683737</td>\n",
       "      <td>294.402746</td>\n",
       "      <td>2829.227380</td>\n",
       "      <td>3962.757279</td>\n",
       "      <td>330.073177</td>\n",
       "      <td>5337.857758</td>\n",
       "      <td>62.468402</td>\n",
       "      <td>8.361420</td>\n",
       "      <td>43.406893</td>\n",
       "      <td>9.966314</td>\n",
       "      <td>6.675758</td>\n",
       "      <td>41.480432</td>\n",
       "      <td>0.026521</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.014419</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>0.169517</td>\n",
       "      <td>1.520725</td>\n",
       "      <td>0.075541</td>\n",
       "      <td>0.101224</td>\n",
       "      <td>0.142332</td>\n",
       "      <td>0.231553</td>\n",
       "      <td>0.747970</td>\n",
       "      <td>-353.813354</td>\n",
       "      <td>129.389587</td>\n",
       "      <td>-2.419618</td>\n",
       "      <td>12.184993</td>\n",
       "      <td>1.073464</td>\n",
       "      <td>4.659044</td>\n",
       "      <td>-17.248865</td>\n",
       "      <td>-12.786888</td>\n",
       "      <td>-16.230808</td>\n",
       "      <td>-19.321218</td>\n",
       "      <td>-11.516044</td>\n",
       "      <td>-9.440626</td>\n",
       "      <td>-6.691416</td>\n",
       "      <td>-6.815610</td>\n",
       "      <td>3211.434326</td>\n",
       "      <td>1468.972778</td>\n",
       "      <td>660.658691</td>\n",
       "      <td>345.593658</td>\n",
       "      <td>328.673828</td>\n",
       "      <td>229.757553</td>\n",
       "      <td>212.703888</td>\n",
       "      <td>117.817871</td>\n",
       "      <td>119.327805</td>\n",
       "      <td>116.079788</td>\n",
       "      <td>99.056915</td>\n",
       "      <td>97.324768</td>\n",
       "      <td>70.701469</td>\n",
       "      <td>72.345901</td>\n",
       "      <td>15.750104</td>\n",
       "      <td>4.063163</td>\n",
       "      <td>0.197638</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>41.972666</td>\n",
       "      <td>0.742968</td>\n",
       "      <td>0.136633</td>\n",
       "      <td>0.317539</td>\n",
       "      <td>8.482101</td>\n",
       "      <td>0.689515</td>\n",
       "      <td>14.683393</td>\n",
       "      <td>0.651029</td>\n",
       "      <td>20.878124</td>\n",
       "      <td>27.530090</td>\n",
       "      <td>0.359157</td>\n",
       "      <td>0.278807</td>\n",
       "      <td>0.105682</td>\n",
       "      <td>0.425248</td>\n",
       "      <td>0.450490</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.290034</td>\n",
       "      <td>0.181226</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.573264</td>\n",
       "      <td>0.660235</td>\n",
       "      <td>0.558399</td>\n",
       "      <td>0.514466</td>\n",
       "      <td>0.170543</td>\n",
       "      <td>0.516760</td>\n",
       "      <td>0.411465</td>\n",
       "      <td>0.519059</td>\n",
       "      <td>0.266270</td>\n",
       "      <td>0.176623</td>\n",
       "      <td>0.082006</td>\n",
       "      <td>19.661740</td>\n",
       "      <td>0.439596</td>\n",
       "      <td>16.361679</td>\n",
       "      <td>0.323915</td>\n",
       "      <td>14.323420</td>\n",
       "      <td>1.335417</td>\n",
       "      <td>28.500505</td>\n",
       "      <td>34.043457</td>\n",
       "      <td>0.098656</td>\n",
       "      <td>0.081444</td>\n",
       "      <td>0.049846</td>\n",
       "      <td>0.059465</td>\n",
       "      <td>0.108205</td>\n",
       "      <td>0.437205</td>\n",
       "      <td>0.086181</td>\n",
       "      <td>0.307087</td>\n",
       "      <td>0.499192</td>\n",
       "      <td>0.233357</td>\n",
       "      <td>0.145322</td>\n",
       "      <td>0.186987</td>\n",
       "      <td>0.087660</td>\n",
       "      <td>0.428962</td>\n",
       "      <td>0.094215</td>\n",
       "      <td>0.145861</td>\n",
       "      <td>0.303827</td>\n",
       "      <td>0.108988</td>\n",
       "      <td>0.104219</td>\n",
       "      <td>0.106198</td>\n",
       "      <td>4.612720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     sex  GHQ_somatic_symptoms  GHQ_anxiety_insomnia  \\\n",
       "0   1    male                     9                     7   \n",
       "1   1    male                     9                     7   \n",
       "2   1    male                     9                     7   \n",
       "3   2  female                    17                    11   \n",
       "4   2  female                    17                    11   \n",
       "\n",
       "   GHQ_social_dysfunction  GHQ_severe_depression  GHQ_total  \\\n",
       "0                      12                      7         35   \n",
       "1                      12                      7         35   \n",
       "2                      12                      7         35   \n",
       "3                      20                     11         59   \n",
       "4                      20                     11         59   \n",
       "\n",
       "   STADI_Trait_agitation  STADI_Trait_worry  STADI_Trait_euthymia  \\\n",
       "0                      7                  7                    18   \n",
       "1                      7                  7                    18   \n",
       "2                      7                  7                    18   \n",
       "3                     10                 10                    12   \n",
       "4                     10                 10                    12   \n",
       "\n",
       "   STADI_Trait_dysthymia  STADI_Trait_total  PSS_total  LEC_exposure  \\\n",
       "0                      7                 32         24             4   \n",
       "1                      7                 32         24             4   \n",
       "2                      7                 32         24             4   \n",
       "3                      5                 37         24             7   \n",
       "4                      5                 37         24             7   \n",
       "\n",
       "   MIMIS_burden  MIMIS_exposure  CTQ_total condition     f0_mean   f0_stddev  \\\n",
       "0            54              54         26   neutral   96.604305  106.481187   \n",
       "1            54              54         26     happy  127.019532   88.005173   \n",
       "2            54              54         26    stress  104.352416   80.628550   \n",
       "3            34              26         40   neutral  124.357467  101.024482   \n",
       "4            34              26         40     happy  135.461952  101.699554   \n",
       "\n",
       "     f0_range     f1_mean   f1_stddev     f1_range      f2_mean   f2_stddev  \\\n",
       "0  497.527041  536.424917  233.754904  2464.771816  1695.857691  296.608117   \n",
       "1  425.839279  535.865682  213.257234  2072.103543  1690.325368  322.428175   \n",
       "2  492.859823  549.499292  195.576501  1973.228026  1652.595697  300.956057   \n",
       "3  499.330235  563.682759  184.679481  2581.450413  1730.330420  301.335320   \n",
       "4  493.071836  551.354266  206.599329  2133.555782  1717.891940  360.038652   \n",
       "\n",
       "      f2_range      f3_mean   f3_stddev     f3_range      f4_mean   f4_stddev  \\\n",
       "0  2733.114515  2791.178599  298.995644  2166.045138  3932.643447  357.076609   \n",
       "1  3054.901427  2763.748723  277.013150  3066.132075  3859.689490  373.231877   \n",
       "2  2752.069793  2756.298906  270.743336  3123.928277  3895.376444  387.713151   \n",
       "3  3046.016603  2809.216860  253.389106  3233.679339  3922.743203  389.741167   \n",
       "4  2821.529345  2752.683737  294.402746  2829.227380  3962.757279  330.073177   \n",
       "\n",
       "      f4_range  loudness_mean  loudness_stddev  loudness_range   hnr_mean  \\\n",
       "0  5397.640299      54.584391         7.335533       36.334180   7.256327   \n",
       "1  5411.600029      53.034700         7.398659       34.115776  11.240782   \n",
       "2  5294.750771      56.983224         7.407861       36.032660   9.277295   \n",
       "3  5333.386736      60.188131         7.761690       40.079754  11.292166   \n",
       "4  5337.857758      62.468402         8.361420       43.406893   9.966314   \n",
       "\n",
       "   hnr_stddev  hnr_range    jitter  jitter_abs  jitter_rap  jitter_ppq5  \\\n",
       "0    5.929404  30.862217  0.035086    0.000157    0.017610     0.023556   \n",
       "1    6.419543  34.996242  0.027139    0.000171    0.014050     0.015352   \n",
       "2    5.459930  34.988818  0.022798    0.000224    0.013123     0.017006   \n",
       "3    6.020655  35.320026  0.022332    0.000116    0.012506     0.013045   \n",
       "4    6.675758  41.480432  0.026521    0.000137    0.014419     0.015352   \n",
       "\n",
       "   jitter_ddp   shimmer  shimmer_db  shimmer_apq3  shimmer_apq5  \\\n",
       "0    0.052829  0.169198    1.577511      0.077511      0.101986   \n",
       "1    0.047259  0.159914    1.442173      0.071833      0.101419   \n",
       "2    0.039460  0.154700    1.434752      0.065842      0.106139   \n",
       "3    0.038756  0.136277    1.333635      0.056344      0.082231   \n",
       "4    0.038632  0.169517    1.520725      0.075541      0.101224   \n",
       "\n",
       "   shimmer_apq11  shimmer_dda  gne_ratio  mfcc1_mean  mfcc2_mean  mfcc3_mean  \\\n",
       "0       0.152519     0.235058   0.737624 -412.656647  125.328049   -5.162876   \n",
       "1       0.147259     0.210641   0.715506 -452.657104  135.154694    1.564904   \n",
       "2       0.183303     0.200735   0.711139 -410.697601  137.463089   -5.219544   \n",
       "3       0.138373     0.176716   0.702688 -363.402191  121.782951   -5.162876   \n",
       "4       0.142332     0.231553   0.747970 -353.813354  129.389587   -2.419618   \n",
       "\n",
       "   mfcc4_mean  mfcc5_mean  mfcc6_mean  mfcc7_mean  mfcc8_mean  mfcc9_mean  \\\n",
       "0   19.649237    2.307594    8.645128  -12.219556  -11.588177  -18.409943   \n",
       "1   17.994049   -0.509994    4.637099   -9.283381   -7.039497  -10.919061   \n",
       "2   22.447281   -2.982574    9.057162   -4.631145  -13.301534  -14.168283   \n",
       "3   13.216275   -5.863396    2.906855  -14.926833  -16.520691  -13.818157   \n",
       "4   12.184993    1.073464    4.659044  -17.248865  -12.786888  -16.230808   \n",
       "\n",
       "   mfcc10_mean  mfcc11_mean  mfcc12_mean  mfcc13_mean  mfcc14_mean  \\\n",
       "0   -18.645103   -10.612516   -13.641768    -6.321719    -4.944730   \n",
       "1   -14.119902   -11.469385   -10.784728    -6.068041    -7.607950   \n",
       "2   -16.331743   -12.601930    -9.062191    -3.341748    -5.156540   \n",
       "3   -23.299971   -12.890169   -12.276170    -6.864491    -8.935539   \n",
       "4   -19.321218   -11.516044    -9.440626    -6.691416    -6.815610   \n",
       "\n",
       "     mfcc1_var    mfcc2_var   mfcc3_var   mfcc4_var   mfcc5_var   mfcc6_var  \\\n",
       "0  2344.435547   790.002625  530.800659  245.813293  313.791046  165.966782   \n",
       "1  1910.520508   700.867432  282.492523  199.471848  176.008224  113.036934   \n",
       "2  2468.172363  1086.665771  315.618103  245.449173  208.679886  147.195374   \n",
       "3  2195.217529  1005.213684  431.079529  385.493713  295.572388  125.941345   \n",
       "4  3211.434326  1468.972778  660.658691  345.593658  328.673828  229.757553   \n",
       "\n",
       "    mfcc7_var   mfcc8_var   mfcc9_var  mfcc10_var  mfcc11_var  mfcc12_var  \\\n",
       "0  140.408020   87.357391   69.960289   93.729332   58.714436   48.906387   \n",
       "1  150.768936   93.646530   60.236416   49.465313   52.001652   39.587254   \n",
       "2   96.288376   66.058853   62.870209   68.456337   53.763882   53.418858   \n",
       "3  242.493271   90.305038   64.390907  119.272141   69.967506   62.288139   \n",
       "4  212.703888  117.817871  119.327805  116.079788   99.056915   97.324768   \n",
       "\n",
       "   mfcc13_var  mfcc14_var   cpp_mean   cpp_var      spir  dur_med   dur_mad  \\\n",
       "0   38.506908   35.027893  15.295692  2.597815  0.201667   0.8245  0.315040   \n",
       "1   47.852798   38.696766  15.027884  2.370696  0.061700   0.6670  0.152167   \n",
       "2   56.459240   43.430695  15.055896  2.248205  0.419921   0.6890  0.063182   \n",
       "3   52.287842   36.745750  15.612209  3.015392  0.381191   0.6940  0.096500   \n",
       "4   70.701469   72.345901  15.750104  4.063163  0.197638   0.8340  0.129000   \n",
       "\n",
       "   silence_ratio  rel_f0_sd  rel_se0_sd  anger_mean  disgust_mean  fear_mean  \\\n",
       "0      57.097033   1.095334    0.133440    2.549506      0.608424   0.078704   \n",
       "1      44.560748   0.739442    0.138167    0.650940      8.482101   0.104265   \n",
       "2      54.503616   0.773881    0.130775    0.708864      7.020399   0.563129   \n",
       "3      46.489476   0.767778    0.128089    0.303205      5.993415   0.274243   \n",
       "4      41.972666   0.742968    0.136633    0.317539      8.482101   0.689515   \n",
       "\n",
       "   happiness_mean  sadness_mean  surprise_mean  neutral_mean  AU01_mean  \\\n",
       "0        6.842891      0.420406       7.288031     82.211540   0.379876   \n",
       "1       32.010799      0.510894       0.246688     31.518047   0.291626   \n",
       "2       31.234692      0.713657       1.046582     58.599697   0.381426   \n",
       "3       33.066822      3.269015       7.288031     39.839859   0.545608   \n",
       "4       14.683393      0.651029      20.878124     27.530090   0.359157   \n",
       "\n",
       "   AU02_mean  AU04_mean  AU05_mean  AU06_mean  AU07_mean  AU09_mean  \\\n",
       "0   0.187983   0.193788   0.287283   0.240030   0.461538   0.191009   \n",
       "1   0.301858   0.116367   0.347408   0.618265   0.468531   0.290034   \n",
       "2   0.282718   0.151235   0.326344   0.667726   0.681592   0.463432   \n",
       "3   0.387015   0.161678   0.245133   0.610585   0.666667   0.336865   \n",
       "4   0.278807   0.105682   0.425248   0.450490   0.555556   0.290034   \n",
       "\n",
       "   AU10_mean  AU11_mean  AU12_mean  AU14_mean  AU15_mean  AU17_mean  \\\n",
       "0   0.078408   0.572464   0.253859   0.501327   0.484803   0.527295   \n",
       "1   0.629238   0.575419   0.794495   0.563615   0.574896   0.407324   \n",
       "2   0.515825   0.601562   0.727933   0.761967   0.577076   0.400158   \n",
       "3   0.407223   0.589520   0.483533   0.500428   0.289023   0.434401   \n",
       "4   0.181226   0.454545   0.573264   0.660235   0.558399   0.514466   \n",
       "\n",
       "   AU20_mean  AU23_mean  AU24_mean  AU25_mean  AU26_mean  AU28_mean  \\\n",
       "0   0.196429   0.495145   0.405418   0.482940   0.382482   0.147255   \n",
       "1   0.473404   0.346378   0.193672   0.787273   0.429520   0.025190   \n",
       "2   0.280702   0.366442   0.269149   0.749651   0.569153   0.042612   \n",
       "3   0.398438   0.363231   0.290635   0.747445   0.352010   0.079542   \n",
       "4   0.170543   0.516760   0.411465   0.519059   0.266270   0.176623   \n",
       "\n",
       "   AU43_mean  mouth_openness_mean  anger_std  disgust_std  fear_std  \\\n",
       "0   0.156937            23.126518   7.225237     1.952985  0.227613   \n",
       "1   0.334740            27.791473   2.189342    35.176575  1.204603   \n",
       "2   0.238092            23.209839   2.455595    11.329882  8.890807   \n",
       "3   0.379027            23.690867   0.536156    16.174517  1.232962   \n",
       "4   0.082006            19.661740   0.439596    16.361679  0.323915   \n",
       "\n",
       "   happiness_std  sadness_std  surprise_std  neutral_std  AU01_std  AU02_std  \\\n",
       "0      24.319393     2.266029      5.975174    35.813168  0.102583  0.050298   \n",
       "1      34.631630     2.453246      5.760518    37.301800  0.079605  0.100976   \n",
       "2      40.812943     1.911544      8.728185    38.862835  0.071536  0.084453   \n",
       "3      33.196033     2.266029     23.911682    38.218197  0.089155  0.090863   \n",
       "4      14.323420     1.335417     28.500505    34.043457  0.098656  0.081444   \n",
       "\n",
       "   AU04_std  AU05_std  AU06_std  AU07_std  AU09_std  AU10_std  AU11_std  \\\n",
       "0  0.115985  0.051084  0.170119  0.274974  0.103052  0.140787  0.489667   \n",
       "1  0.049846  0.094841  0.267469  0.470088  0.103786  0.341909  0.452735   \n",
       "2  0.082285  0.062929  0.206310  0.495407  0.129207  0.326406  0.479352   \n",
       "3  0.080697  0.034277  0.228250  0.458867  0.090659  0.354915  0.484948   \n",
       "4  0.049846  0.059465  0.108205  0.437205  0.086181  0.307087  0.499192   \n",
       "\n",
       "   AU12_std  AU14_std  AU15_std  AU17_std  AU20_std  AU23_std  AU24_std  \\\n",
       "0  0.208955  0.210862  0.152521  0.105481  0.470924  0.135590  0.205980   \n",
       "1  0.268431  0.093725  0.159801  0.102256  0.500625  0.162797  0.149417   \n",
       "2  0.299969  0.114262  0.158375  0.081024  0.471398  0.112794  0.115976   \n",
       "3  0.240953  0.130662  0.153249  0.088911  0.477791  0.109224  0.165184   \n",
       "4  0.233357  0.145322  0.186987  0.087660  0.428962  0.094215  0.145861   \n",
       "\n",
       "   AU25_std  AU26_std  AU28_std  AU43_std  mouth_openness_std  \n",
       "0  0.391346  0.189271  0.104020  0.224458            4.779605  \n",
       "1  0.380655  0.188924  0.029636  0.280063            5.905929  \n",
       "2  0.267835  0.137593  0.018114  0.269494            6.210818  \n",
       "3  0.334715  0.174493  0.036487  0.323725            5.725777  \n",
       "4  0.303827  0.108988  0.104219  0.106198            4.612720  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"../simulated_processed_data/simulated_data_combined.csv\")\n",
    "# Preview the dataset (first few rows)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a698e3",
   "metadata": {},
   "source": [
    "### Task 2: Create a function to preprocess the data\n",
    "\n",
    "Follow the steps:\n",
    "1. Drop metadata columns\n",
    "2. Separate features and target\n",
    "3. Encode target labels (sklearn - LabelEncoder)\n",
    "4. Split into train and test sets (sklearn - train_test_split)\n",
    "5. Impute missing values (sklearn - SimpleImputer)\n",
    "6. Scale features (sklearn - StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f67b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function for preprocessing\n",
    "def preprocess_data(df, target_col=\"condition\",test_size=0.2, random_state=42, metadata_cols=[\"id\", \"sex\"]):\n",
    "    \"\"\"\n",
    "        Preprocess the dataset for modeling:\n",
    "        - Drop metadata columns\n",
    "        - Separate features and target\n",
    "        - Encode target labels\n",
    "        - Split into train and test sets\n",
    "        - Impute missing values (fit on train, transform on test)\n",
    "        - Scale features (fit on train, transform on test)\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataset\n",
    "            target_col (str): Name of the target column\n",
    "            metadata_cols (list, optional): Columns to drop\n",
    "            test_size (float, optional): Proportion of the dataset to include in the test split\n",
    "            random_state (int, optional): Seed used by the random number generator\n",
    "\n",
    "        Returns:\n",
    "            X_train_scaled (np.ndarray): Scaled training feature matrix\n",
    "            X_test_scaled (np.ndarray): Scaled test feature matrix\n",
    "            y_train (np.ndarray): Encoded training target labels\n",
    "            y_test (np.ndarray): Encoded test target labels\n",
    "            feature_names (list): Names of the features\n",
    "            class_labels (np.ndarray): Original class labels\n",
    "    \"\"\"\n",
    "    return None, None, None, None, None, None\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2ab4c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"font-size:20px; color:darkgoldenrod; font-weight:bold;\">Click to see the solution</span></summary>\n",
    "\n",
    "```python\n",
    "def preprocess_data(df, target_col=\"condition\",test_size=0.2, random_state=42, metadata_cols=[\"id\", \"sex\"]):\n",
    "    \"\"\"\n",
    "        Preprocess the dataset for modeling:\n",
    "        - Drop metadata columns\n",
    "        - Separate features and target\n",
    "        - Encode target labels\n",
    "        - Split into train and test sets\n",
    "        - Impute missing values (fit on train, transform on test)\n",
    "        - Scale features (fit on train, transform on test)\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataset\n",
    "            target_col (str): Name of the target column\n",
    "            metadata_cols (list, optional): Columns to drop\n",
    "            test_size (float, optional): Proportion of the dataset to include in the test split\n",
    "            random_state (int, optional): Seed used by the random number generator\n",
    "\n",
    "        Returns:\n",
    "            X_train_scaled (np.ndarray): Scaled training feature matrix\n",
    "            X_test_scaled (np.ndarray): Scaled test feature matrix\n",
    "            y_train (np.ndarray): Encoded training target labels\n",
    "            y_test (np.ndarray): Encoded test target labels\n",
    "            feature_names (list): Names of the features\n",
    "            class_labels (np.ndarray): Original class labels\n",
    "    \"\"\"\n",
    "    #return None, None, None, None, None, None\n",
    "    if metadata_cols is None:\n",
    "        metadata_cols = []\n",
    "\n",
    "    # Drop metadata columns \n",
    "    df = df.drop(columns=metadata_cols, errors=\"ignore\")\n",
    "\n",
    "    # Separate target and features\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Encode target labels\n",
    "    label_enc = LabelEncoder()\n",
    "    y_enc = label_enc.fit_transform(y)\n",
    "    class_labels = label_enc.classes_\n",
    "\n",
    "    # Split into train and test (before scaling/imputation to avoid leakage)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, test_size=test_size, random_state=random_state, stratify=y_enc\n",
    "    )\n",
    "\n",
    "    # Impute missing values (fit on training, transform both)\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "    # Scale features (fit on training, transform both)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_names, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db355c62",
   "metadata": {},
   "source": [
    "### Task 3: Elastic Net Logistic Regression\n",
    "\n",
    " Fit multinomial logistic regression with Elastic Net penalty (sklearn - LogosticRegression)\n",
    "\n",
    "Logistic Regression is suitable for classification tasks, including multinomial problems like predicting multiple categories\n",
    "\n",
    "Elastic Net combines L1 (Lasso) and L2 (Ridge) regularization: [original paper: https://academic.oup.com/jrsssb/article/67/2/301/7109482]\n",
    "1. L1 encourages sparsity, helping to select important features and ignore irrelevant ones.\n",
    "2. L2 shrinks coefficients to prevent overfitting and improve model generalization.\n",
    "\n",
    " Important: Make sure to split the data into train and test sets, and use only the training set to identify feature importanceotherwise, you risk data leakage for the next classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ebb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function to for elastic net logisitic regression\n",
    "def elasticnet_logreg(X_scaled, y_enc, feature_names, class_labels, l1_ratio=0.5, max_iter=5000):\n",
    "    \"\"\"\n",
    "    Fit multinomial logistic regression with Elastic Net and return coefficients\n",
    "\n",
    "    Args:\n",
    "        X_scaled (np.ndarray): Preprocessed features \n",
    "        y_enc (np.ndarray): Encoded target labels\n",
    "        feature_names (list): Names of the features\n",
    "        class_labels (np.ndarray): Original class labels\n",
    "        l1_ratio (float): Elastic Net mixing parameter\n",
    "        max_iter (int): Maximum iterations for convergence\n",
    "\n",
    "    Returns:\n",
    "        coef_df (pd.DataFrame): Feature coefficients (features x classes)\n",
    "    \"\"\"\n",
    "    return None   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950bdb7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"font-size:20px; color:darkgoldenrod; font-weight:bold;\">Click to see the solution</span></summary>\n",
    "\n",
    "```python\n",
    "def elasticnet_logreg(X_scaled, y_enc, feature_names, class_labels, l1_ratio=0.5, max_iter=5000):\n",
    "    \"\"\"\n",
    "    Fit multinomial logistic regression with Elastic Net and return coefficients\n",
    "\n",
    "    Args:\n",
    "        X_scaled (np.ndarray): Preprocessed features \n",
    "        y_enc (np.ndarray): Encoded target labels\n",
    "        feature_names (list): Names of the features\n",
    "        class_labels (np.ndarray): Original class labels\n",
    "        l1_ratio (float): Elastic Net mixing parameter\n",
    "        max_iter (int): Maximum iterations for convergence\n",
    "\n",
    "    Returns:\n",
    "        coef_df (pd.DataFrame): Feature coefficients (features x classes)\n",
    "    \"\"\"\n",
    "    #return None\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        l1_ratio=l1_ratio,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        multi_class=\"multinomial\"\n",
    "    )\n",
    "    clf.fit(X_scaled, y_enc)\n",
    "\n",
    "    coef_df = pd.DataFrame(clf.coef_.T, index=feature_names, columns=class_labels)\n",
    "    return coef_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d9127",
   "metadata": {},
   "source": [
    "### Task 4: Plot Top Features\n",
    "\n",
    "Visualize the top N features (by absolute coefficient) per emotion class using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb6fcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef_heatmap(coef_df, N=20, figsize=(10, 8), cmap=\"coolwarm\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of the top N features (by absolute coefficient across classes).\n",
    "\n",
    "    Args:\n",
    "        coef_df (pd.DataFrame): Feature coefficients (features  classes)\n",
    "        dataset_name (str): Dataset name for title\n",
    "        N (int): Number of top features to display\n",
    "        figsize (tuple): Figure size\n",
    "        cmap (str): Colormap for heatmap\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Select top-N features overall\n",
    "        top_features = coef_df.abs().max(axis=1).nlargest(N).index\n",
    "        plot_df = coef_df.loc[top_features]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(\n",
    "            plot_df,\n",
    "            annot=True, fmt=\".2f\", cmap=cmap, center=0,\n",
    "            cbar_kws={\"label\": \"Coefficient\"}\n",
    "        )\n",
    "        plt.title(f\"Top {N}  Features (Elastic Net Logistic Regression)\", fontsize=14)\n",
    "        plt.xlabel(\"Emotion Class\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Could not plot\")\n",
    "        print(\"Most likely the code is not yet complete (complete the cells with TODO).\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31df42",
   "metadata": {},
   "source": [
    "### Task 5: Apply Model \n",
    "\n",
    "Fit model and plot top features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90f88ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test, feature_names, class_labels = preprocess_data(df)\n",
    "coefs = elasticnet_logreg(X_train_scaled, y_train, feature_names, class_labels, l1_ratio=0.5)\n",
    "\n",
    "plot_coef_heatmap(coefs, N=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49389545",
   "metadata": {},
   "source": [
    "### Insights?\n",
    "\n",
    " Which features have the largest absolute coefficients for each emotion? Do these make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b7978",
   "metadata": {},
   "source": [
    "### Task 6: Identify features with zero coefficient value. \n",
    "\n",
    "Relevant for the next script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5349ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    elasticnet_abs = coefs.abs().max(axis=1)\n",
    "    elasticnet_dropped_features = elasticnet_abs[elasticnet_abs == 0].index.tolist()\n",
    "\n",
    "    print(\"features dropped :\", len(elasticnet_dropped_features), elasticnet_dropped_features)\n",
    "except Exception as e:\n",
    "    print(\"Most likely the code is not yet complete (complete the cells with TODO).\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c54de",
   "metadata": {},
   "source": [
    "#### Bonus task\n",
    " Try experimenting with different l1_ratio values to see how it affects feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e7870",
   "metadata": {},
   "source": [
    "# 2. Mental Health Survey: Predicting GHQ Scores under Stress\n",
    "\n",
    "In this analysis, we aim to identify which features are most predictive of participants' **GHQ_total** scores under the stress condition.  \n",
    "\n",
    "We use **Elastic Net Regression** with cross-validation, which combines L1 (Lasso) and L2 (Ridge) penalties to perform feature selection while accounting for correlated predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afeeb48",
   "metadata": {},
   "source": [
    "## Step 1: Load and Filter Data\n",
    "\n",
    "We first load the dataset and focus on participants under the stress condition.  \n",
    "\n",
    "- GHQ_total is the target variable (outcome measure) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c8d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "df = pd.read_csv(\"../simulated_processed_data/simulated_data_combined.csv\")\n",
    "\n",
    "df_stress = df[df[\"condition\"] == \"stress\"].copy()\n",
    "\n",
    "# Define target and features\n",
    "y = df_stress[\"GHQ_total\"]\n",
    "X = df_stress.drop(columns=[\"id\", \"sex\", \"condition\", \"GHQ_total\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279db71",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d577d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add code in place of ...\n",
    "# Impute missing values\n",
    "...\n",
    "\n",
    "# Scale features \n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7282e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"font-size:20px; color:darkgoldenrod; font-weight:bold;\">Click to see the solution</span></summary>\n",
    "\n",
    "```python\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916fdc2",
   "metadata": {},
   "source": [
    "## Step 3: Fit Elastic Net Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49805fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add code in place of ... \n",
    "# Fit Elastic Net with CV \n",
    "elastic_net = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91f14f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"font-size:20px; color:darkgoldenrod; font-weight:bold;\">Click to see the solution</span></summary>\n",
    "\n",
    "```python\n",
    "ElasticNetCV(l1_ratio=.5, \n",
    "            alphas=np.arange(0.1, 1.1, 0.1),\n",
    "            cv=5,\n",
    "            max_iter=5000,\n",
    "            random_state=42)\n",
    "elastic_net.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1bd7a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get feature importance (coefficients)\n",
    "    coef = pd.Series(elastic_net.coef_, index=X.columns)\n",
    "\n",
    "    # Sort by absolute importance\n",
    "    important_features = coef[coef != 0].sort_values(key=np.abs, ascending=False)\n",
    "\n",
    "    print(\"Optimal alpha:\", elastic_net.alpha_)\n",
    "    print(\"\\nTop features influencing GHQ_total:\")\n",
    "    print(len(important_features))\n",
    "    display(important_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Most likely the code is not yet complete (complete the cells with TODO).\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce76d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
